{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbM8WM9qv8ju"
   },
   "source": [
    "# Exploratory Pipeline for the Cognitive Science and Artificial Intelligence thesis \"The Key Motif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzkgw003jxzf"
   },
   "source": [
    "Zenodo repository:\n",
    "https://zenodo.org/records/11061100\n",
    "\n",
    "Files needed:\n",
    "\n",
    "- _\"Locibase.json\"_\n",
    "- _\"esm2_embeddings_rbp.csv\"_\n",
    "- _\"phage_host_interactions.csv\"_\n",
    "- _\"RBPbase.csv\"_\n",
    "\n",
    "\n",
    "Files generated:\n",
    "- _\"esm2_embeddings_loci_per_protein.csv\"_ <br>\n",
    "Contains the host protein embeddings for each locus protein\n",
    "<br>\n",
    "- _\"all_interactions_no_embeddings.csv\"_ <br>\n",
    "Contains phage-host interactions, without ESM-2 embeddings (to make it lighter) <br>\n",
    "- _\"kaptive_results.tsv\"_ <br>\n",
    "Contains K-loci information for each host, extracted using Kaptive <br>\n",
    "- _\"protein_sequences_K#_positive.fasta\"_ <br>\n",
    "Contains the aminoacid sequences of the Receptor-Binding Proteins of all phages that infect hosts of the K# locus, concatenated per phage <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G60EOrHpac2P"
   },
   "source": [
    "# Obtaining individual host proteins\n",
    "\n",
    "generates \"esm2_embeddings_loci_per_protein.csv\" from \"Locibase.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a2DgWmUaJoN",
    "outputId": "1bf1f2fc-819a-4ab9-b5f6-6998e703bbbc"
   },
   "outputs": [],
   "source": [
    "!pip install fair-esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hwhWCU9IaJgt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding loci proteins:   8%|████▌                                                 | 17/200 [19:02<3:24:58, 67.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings_df\n\u001b[32m     68\u001b[39m loci_path = \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# \"path_to_folder_containing_Locibase.json\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[43mcompute_esm2_embeddings_loci_per_protein\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloci_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mcompute_esm2_embeddings_loci_per_protein\u001b[39m\u001b[34m(general_path, data_suffix, add)\u001b[39m\n\u001b[32m     43\u001b[39m batch_labels, batch_strs, batch_tokens = batch_converter(data)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     results = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepr_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m33\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_contacts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m token_representations = results[\u001b[33m\"\u001b[39m\u001b[33mrepresentations\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m33\u001b[39m]\n\u001b[32m     47\u001b[39m protein_embedding = token_representations[\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m : \u001b[38;5;28mlen\u001b[39m(sequence) + \u001b[32m1\u001b[39m].mean(\u001b[32m0\u001b[39m).numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\paper\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\paper\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\paper\\Lib\\site-packages\\esm\\model\\esm2.py:112\u001b[39m, in \u001b[36mESM2.forward\u001b[39m\u001b[34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[39m\n\u001b[32m    109\u001b[39m     padding_mask = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers):\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     x, attn = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (layer_idx + \u001b[32m1\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m repr_layers:\n\u001b[32m    118\u001b[39m         hidden_representations[layer_idx + \u001b[32m1\u001b[39m] = x.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\paper\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\paper\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import esm\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_esm2_embeddings_loci_per_protein(general_path, data_suffix='', add=False):\n",
    "    \"\"\"\n",
    "    This function computes ESM-2 embeddings for each individual protein within loci, from the Locibase.json file.\n",
    "\n",
    "    INPUTS:\n",
    "    - general path to the project data folder\n",
    "    - data suffix to optionally add to the saved file name (default='')\n",
    "    OUTPUT: esm2_embeddings_loci_per_protein.csv (with one embedding per protein)\n",
    "    \"\"\"\n",
    "\n",
    "    # Load ESM-2 model\n",
    "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "    # Load json file\n",
    "    with open(general_path + '/Locibase' + data_suffix + '.json') as dict_file:\n",
    "        loci_dict = json.load(dict_file)\n",
    "\n",
    "    # if embeddings already exist, to append new ones to them\n",
    "    if add:\n",
    "        old_embeddings_df = pd.read_csv(general_path + '/esm2_embeddings_loci_per_protein' + data_suffix + '.csv')\n",
    "        processed_accession_proteins = set(zip(old_embeddings_df['accession'], old_embeddings_df['protein_index']))\n",
    "        for key in list(loci_dict.keys()):\n",
    "            loci_dict[key] = [seq for i, seq in enumerate(loci_dict[key]) if (key, i) not in processed_accession_proteins]\n",
    "        print('Processing', sum(len(v) for v in loci_dict.values()), 'more protein sequences (add=True)')\n",
    "\n",
    "    # Compute embeddings per protein\n",
    "    protein_representations = []\n",
    "    accessions = []\n",
    "    protein_indices = []\n",
    "\n",
    "    for key in tqdm(loci_dict.keys(), desc=\"Embedding loci proteins\"):\n",
    "        for idx, sequence in enumerate(loci_dict[key]):\n",
    "            data = [(f\"{key}_prot_{idx}\", sequence)]\n",
    "            batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "            with torch.no_grad():\n",
    "                results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "            token_representations = results[\"representations\"][33]\n",
    "            protein_embedding = token_representations[0, 1 : len(sequence) + 1].mean(0).numpy()\n",
    "\n",
    "            accessions.append(key)\n",
    "            protein_indices.append(idx)\n",
    "            protein_representations.append(protein_embedding)\n",
    "\n",
    "    # Save results\n",
    "    embeddings_df = pd.concat([\n",
    "        pd.DataFrame({'accession': accessions, 'protein_index': protein_indices}),\n",
    "        pd.DataFrame(protein_representations)\n",
    "    ], axis=1)\n",
    "\n",
    "    if add:\n",
    "        embeddings_df = pd.concat([old_embeddings_df, embeddings_df], axis=0, ignore_index=True)\n",
    "\n",
    "    embeddings_df.to_csv(general_path + '/esm2_embeddings_loci_per_protein' + data_suffix + '.csv', index=False)\n",
    "    print(\"Saved embeddings to:\", general_path + '/esm2_embeddings_loci_per_protein' + data_suffix + '.csv')\n",
    "\n",
    "    return embeddings_df\n",
    "\n",
    "\n",
    "loci_path = \".\" # \"path_to_folder_containing_Locibase.json\"\n",
    "\n",
    "compute_esm2_embeddings_loci_per_protein(loci_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duWHh1g_Z7u4"
   },
   "source": [
    "# Obtaining confirmed infections-only dataset ('all_interactions_no_embeddings.csv')\n",
    "\n",
    "generates 'all_interactions_no_embeddings.csv' from 'esm2_embeddings_loci_per_protein.csv', 'esm2_embeddings_rbp.csv' and 'phage_host_interactions.csv'\n",
    "\n",
    "adds to it the protein sequences from \"RBPbase.csv\", to generate \"all_infections.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFYeUwmiZ5_t"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os.path\n",
    "\n",
    "\n",
    "\n",
    "embeddings_loci_protein = pd.read_csv(\"esm2_embeddings_loci_per_protein.csv\") # generated above\n",
    "embeddings_rbp = pd.read_csv(\"esm2_embeddings_rbp.csv\")\n",
    "phage_host_interactions = pd.read_csv('phage_host_interactions.csv')\n",
    "\n",
    "# Create a single dataset that has host, phage, and interactions, but not embeddings\n",
    "\n",
    "interactions_melted = phage_host_interactions.melt(\n",
    "    id_vars=['Unnamed: 0'], var_name='phage_ID', value_name='label'\n",
    ").rename(columns={'Unnamed: 0': 'accession'})\n",
    "\n",
    "interactions_melted = interactions_melted.dropna(subset=['label'])\n",
    "\n",
    "merged = interactions_melted.merge(embeddings_loci_protein, on='accession', how='inner')\n",
    "merged = merged.merge(embeddings_rbp, on='phage_ID', how='inner')\n",
    "\n",
    "final_df = merged[['accession', 'phage_ID', 'protein_ID', \"label\"]]\n",
    "\n",
    "print(len(final_df))\n",
    "final_df.drop_duplicates(inplace=True)\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "print(len(final_df))\n",
    "\n",
    "final_df.to_csv('all_interactions_no_embeddings.csv', index=False)\n",
    "print(\"Final per-protein dataframe saved as 'all_interactions_no_embeddings.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYqk0javav8P"
   },
   "outputs": [],
   "source": [
    "# adds Receptor-Binding Proteins to the interactions (no embeddings) file\n",
    "interactions_no_embeddings = pd.read_csv(\"all_interactions_no_embeddings.csv\")\n",
    "RBProteins = pd.read_csv(\"RBPbase.csv\")\n",
    "\n",
    "RBProteins = RBProteins[[\"protein_ID\", \"protein_sequence\"]]\n",
    "RBProteins.head()\n",
    "\n",
    "proteins_no_embeddings = pd.merge(interactions_no_embeddings, RBProteins, how = \"left\", left_on = \"protein_ID\", right_on = \"protein_ID\")\n",
    "\n",
    "proteins_no_embeddings.to_csv(\"all_infections.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-ZEN5aVawfj"
   },
   "source": [
    "# Using Kaptive to determine K-loci\n",
    "\n",
    "requires the download and unzipping of \"klebsiella_genomes.zip\"\n",
    "\n",
    "generates \"kaptive_results.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpJQaNUtawDr"
   },
   "outputs": [],
   "source": [
    "!pip install kaptive\n",
    "!apt-get install minimap2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rB2SlylCawIn"
   },
   "outputs": [],
   "source": [
    "# K-LOCUS EXTRACTION:\n",
    "\n",
    "!kaptive assembly kpsc_k /content/drive/MyDrive/PhageHostLearn_Data/Kaptive/fasta_files/*.fasta -o kaptive_results.tsv -j -p\n",
    "\n",
    "# 8mins to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCCY_DjZbCJc"
   },
   "source": [
    "# Downloading concatenated RBPs of phages that infect specific K-loci as fasta files\n",
    "\n",
    "requires \"all_infections.csv\" and \"kaptive_results.tsv\"\n",
    "generates a .fasta file that contains for each phage infecting a host that belongs to a certain K-locus its proteins, concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_byr0zspa6N3",
    "outputId": "742b16f0-2bde-4489-93dd-087c5efccced"
   },
   "outputs": [],
   "source": [
    "!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "hRI3MSEosT--",
    "outputId": "ad660a22-a493-4f5b-b99b-e945cd68ab03"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_sero = pd.read_csv(\"kaptive_results.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_sero.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpqv-4YlsYLt",
    "outputId": "83dce056-58b1-49da-c198-c17de141fa3d"
   },
   "outputs": [],
   "source": [
    "df_sero[\"Best match type\"].nunique()\n",
    "# 1280 + 87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ZMkIC4psgc6",
    "outputId": "8e3ae198-12b9-4c11-9426-fcb0f7de0421"
   },
   "outputs": [],
   "source": [
    "1280 + 87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPm4i9_c3X7x"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_infections = pd.read_csv(\"all_infections.csv\")\n",
    "df_sero = pd.read_csv(\"kaptive_results.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "CehH1Z4y3X7z",
    "outputId": "29b490d9-0e80-46a8-fa2e-d2d2a93a25a2"
   },
   "outputs": [],
   "source": [
    "all_infections.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "7C3JeOhj3X70",
    "outputId": "6d973a73-caae-4774-8e6c-b7a90a0eb56f"
   },
   "outputs": [],
   "source": [
    "# Combine the infections information with the K-loci information\n",
    "# (\"Best match type\" refers to the K-locus serotype of the host with that \"accession\")\n",
    "df_sero = df_sero[[\"Assembly\", \"Best match type\", \"Match confidence\"]]\n",
    "\n",
    "sero_phage = pd.merge(all_infections, df_sero, how = \"left\", left_on = \"accession\", right_on=\"Assembly\").drop(\"Assembly\", axis=1)\n",
    "\n",
    "sero_phage = sero_phage[sero_phage[\"Match confidence\"] == \"Typeable\"]\n",
    "\n",
    "sero_phage = sero_phage[sero_phage[\"Best match type\"] != \"Capsule null\"]\n",
    "\n",
    "\n",
    "sero_phage.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-b7h45OrvN7W",
    "outputId": "e74a2209-b3da-4855-a45b-607f438bf255"
   },
   "outputs": [],
   "source": [
    "sero_phage[sero_phage[\"label\"] == 1][\"Best match type\"].value_counts()[:10]\n",
    "\n",
    "\n",
    "for el in list(sero_phage[sero_phage[\"label\"] == 1][\"Best match type\"].value_counts().index)[1:]:\n",
    "  df_onehost_analyze = sero_phage[(sero_phage[\"Best match type\"] == el) & (sero_phage[\"label\"] == 1)]\n",
    "\n",
    "  # Only show K-loci serotypes with 7 unique infecting phages\n",
    "  if df_onehost_analyze.phage_ID.nunique() == 7:\n",
    "    print(el)\n",
    "    print(\"Number of Phage proteins: \", len(df_onehost_analyze))\n",
    "    print(\"Number of Phages: \", df_onehost_analyze.phage_ID.nunique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PN-myu0_3X75",
    "outputId": "128e3bea-2b74-44c9-c2dd-bc1ee76fd47d"
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "# pos = 1: Positive phages\n",
    "# pos = 0: Negative phages\n",
    "\n",
    "# Re-run for each K-locus for which enough phages have been identified to generate results\n",
    "serot = \"K13\"\n",
    "\n",
    "# Only check confirmed positive interactions, as even confirmed negatives could\n",
    "# have failed for a number of reasons beyond the initial attachment\n",
    "for pos in [1]:\n",
    "  # Change the label check from 0 to 1 and rename the output to switch from positive to negative interactions\n",
    "  df_onehost = sero_phage[(sero_phage[\"Best match type\"] == serot) & (sero_phage[\"label\"] == pos)]\n",
    "\n",
    "  print(\"Number of Phage proteins: \", len(df_onehost))\n",
    "  print(\"Number of Phages: \", df_onehost.phage_ID.nunique())\n",
    "\n",
    "  # Only keep the first occurrence of each protein per phage\n",
    "  df_onehost = df_onehost.drop_duplicates(subset=[\"phage_ID\", \"protein_sequence\"], keep=\"first\")\n",
    "\n",
    "  print(\"Number of Unique Phage-Proteins couples: \", len(df_onehost))\n",
    "  print(\"Number of Unique Proteins: \", df_onehost.protein_sequence.nunique())\n",
    "\n",
    "\n",
    "  # Group by phage_ID and concatenate protein sequences\n",
    "  df_grouped = df_onehost.groupby(\"phage_ID\")[\"protein_sequence\"].apply(lambda x: \"\".join(x)).reset_index()\n",
    "\n",
    "  # Rename column for clarity\n",
    "  df_grouped.rename(columns={\"protein_sequence\": \"concatenated_sequence\"}, inplace=True)\n",
    "\n",
    "  continue\n",
    "\n",
    "  # Specify the output FASTA file name\n",
    "  if pos:\n",
    "    output_fasta = \"protein_sequences_\" + str(serot) + \"_positive.fasta\"\n",
    "  else:\n",
    "    output_fasta = \"protein_sequences_\" + str(serot) + \"_negative.fasta\"\n",
    "\n",
    "  # Convert DataFrame to SeqRecord format\n",
    "  records = [SeqRecord(Seq(seq), id=protein_id, description=\"\")\n",
    "            for protein_id, seq in zip(df_grouped[\"phage_ID\"], df_grouped[\"concatenated_sequence\"])]\n",
    "\n",
    "  # Write to FASTA file\n",
    "  SeqIO.write(records, output_fasta, \"fasta\")\n",
    "\n",
    "  print(f\"FASTA file saved as '{output_fasta}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbACq11ymD5z"
   },
   "source": [
    "## Load the .fasta file generated above to MEME, specifying \"One Occurrence Per Sequence\" and \"99\" as Maximum Width (under Advanced Options).\n",
    "\n",
    "\n",
    "https://meme-suite.org/meme/\n",
    "\n",
    "Create a folder \"MEME_results\". Within, create a subfolder with the name of the K-locus being analyzed (es: K11).\n",
    "\n",
    "Once a job has been run:\n",
    "- Go to \"MEME HTML output\"\n",
    "- Look for the motif with the lowest E-value\n",
    "- Submit/Download -> Download Motif\n",
    "- Download both \"fasta\" (downloads a .txt that has easily readable information about the position of the motif) and \"Minimal MEME\" (downloads a .meme has full output information)\n",
    "- Save them into \"MEME_results/K#/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuFJoNjNmtRC"
   },
   "source": [
    "# Identify in which protein the motif identified resides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qUGDlYRf0meh",
    "outputId": "5f1fe6c9-4a9d-470d-aef5-8aa2c7fab95b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"MEME_results\"\n",
    "# Path to the \"MEME_results\" folder. Inside one folder for each K-locus.\n",
    "# Inside .meme and .txt files downloaded from MEME for most common motif found\n",
    "\n",
    "subfolders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "\n",
    "print(subfolders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "BD47hNOjsx01",
    "outputId": "1f3978ff-4bc3-447c-a274-5353511c2492"
   },
   "outputs": [],
   "source": [
    "path = \"MEME_results/\" +  serot\n",
    "\n",
    "extension = \".txt\"\n",
    "\n",
    "files = [f for f in os.listdir(path) if f.endswith(extension)]\n",
    "print(files)\n",
    "\n",
    "# Path to your file\n",
    "file_path = os.path.join(path, files[0])\n",
    "\n",
    "# Initialize a list to store extracted data\n",
    "extracted_data = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Process the file line by line\n",
    "for i in range(len(lines)):\n",
    "    line = lines[i].strip()\n",
    "\n",
    "    # Check if the line starts with \">\"\n",
    "    if line.startswith(\">\"):\n",
    "        # Extract the identifier before \"_\"\n",
    "        identifier = line.split(\"_\")[0][1:]  # Remove \">\"\n",
    "\n",
    "        # Extract the offset number using regex\n",
    "        offset_match = re.search(r\"offset=\\s*(\\d+)\", line)\n",
    "        offset = offset_match.group(1) if offset_match else None\n",
    "\n",
    "        # Get the next line (sequence) and compute its length\n",
    "        if i + 1 < len(lines):\n",
    "            sequence = lines[i + 1].strip()\n",
    "            seq_length = len(sequence)\n",
    "\n",
    "        # Store extracted data\n",
    "        extracted_data.append((identifier, offset, seq_length, sequence))\n",
    "\n",
    "# Display results\n",
    "all_extracted = pd.DataFrame(extracted_data, columns=[\"Identifier\", \"Offset\", \"Sequence Length\", \"motif\"])\n",
    "\n",
    "all_extracted.Offset = all_extracted.Offset.apply(lambda x: int(x))\n",
    "\n",
    "# Information extracted from the fasta (.txt) files\n",
    "all_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "AHAMnPg_wwFU",
    "outputId": "600e0bc0-717c-4716-8894-247e183702ee"
   },
   "outputs": [],
   "source": [
    "# Function to compute sequential protein identifier per phage_ID\n",
    "def compute_sequential_identifier(df):\n",
    "    df[\"sequential_protein_identifier\"] = \"\"  # Initialize the column\n",
    "    start_pos = {}  # Dictionary to track start positions for each phage_ID\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        phage_id = row[\"phage_ID\"]\n",
    "        seq_length = len(row[\"protein_sequence\"])\n",
    "\n",
    "        # Get the starting position for this phage_ID\n",
    "        start = start_pos.get(phage_id, 0)\n",
    "        end = start + seq_length\n",
    "\n",
    "        # Assign the range as a string\n",
    "        df.at[index, \"sequential_protein_identifier\"] = f\"{start}-{end}\"\n",
    "\n",
    "        # Update the start position for the next protein under the same phage_ID\n",
    "        start_pos[phage_id] = end\n",
    "\n",
    "    return df\n",
    "\n",
    "df_onehost[\"lengths\"] = df_onehost[\"protein_sequence\"].apply(lambda x: len(x))\n",
    "\n",
    "# Apply the function\n",
    "df_onehost_length = compute_sequential_identifier(df_onehost)\n",
    "\n",
    "# Display the first few rows\n",
    "df_onehost_length.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 982
    },
    "id": "wDdLWbBJy6He",
    "outputId": "3506395e-d6ca-4648-f43e-7122424d49c8"
   },
   "outputs": [],
   "source": [
    "# Initialize the new column as False\n",
    "df_onehost_length[\"motif_found\"] = False\n",
    "\n",
    "# Iterate over extracted_df and find matching rows in df\n",
    "for _, row in all_extracted.iterrows():\n",
    "    identifier = row[\"Identifier\"]\n",
    "    offset = row[\"Offset\"]\n",
    "    seq_length = row[\"Sequence Length\"]\n",
    "    target_range = (offset, offset + seq_length)  # Range for motif\n",
    "\n",
    "    # Filter rows where phage_ID matches Identifier\n",
    "    matching_rows = df_onehost_length[df_onehost_length[\"phage_ID\"] == identifier]\n",
    "\n",
    "    # Iterate through matching rows to check range inclusion\n",
    "    for index, match_row in matching_rows.iterrows():\n",
    "        start, end = map(int, match_row[\"sequential_protein_identifier\"].split(\"-\"))\n",
    "\n",
    "        # Check if the target range is within this row's sequential identifier range\n",
    "        if start <= target_range[0] and end >= target_range[1]:\n",
    "            df_onehost_length.at[index, \"motif_found\"] = True\n",
    "            break  # Ensure only one row is marked True per Identifier\n",
    "\n",
    "df_onehost_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 982
    },
    "id": "JLv4JPSDvA-T",
    "outputId": "8e9afe23-b7a0-4084-d079-d66d1b8b1b85"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "full_onehost_length = copy.deepcopy(df_onehost_length)\n",
    "\n",
    "full_onehost_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vopw_bnE6Bg5"
   },
   "outputs": [],
   "source": [
    "all_extracted.Offset = all_extracted.Offset.apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mi4XnU580836",
    "outputId": "c0a98420-94bf-45cb-d329-1687152531ea"
   },
   "outputs": [],
   "source": [
    "folder_path = \"/motifs_KO\"\n",
    "\n",
    "subfolders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "\n",
    "print(subfolders)\n",
    "\n",
    "# Function to compute sequential protein identifier per phage_ID\n",
    "def compute_sequential_identifier(df):\n",
    "    df[\"sequential_protein_identifier\"] = \"\"  # Initialize the column\n",
    "    start_pos = {}  # Dictionary to track start positions for each phage_ID\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        phage_id = row[\"phage_ID\"]\n",
    "        seq_length = len(row[\"protein_sequence\"])\n",
    "\n",
    "        # Get the starting position for this phage_ID\n",
    "        start = start_pos.get(phage_id, 0)\n",
    "        end = start + seq_length\n",
    "\n",
    "        # Assign the range as a string\n",
    "        df.at[index, \"sequential_protein_identifier\"] = f\"{start}-{end}\"\n",
    "\n",
    "        # Update the start position for the next protein under the same phage_ID\n",
    "        start_pos[phage_id] = end\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "first_iteration = True\n",
    "\n",
    "\n",
    "for serot in [x for x in subfolders if x[0] == \"K\"]:\n",
    "\n",
    "  ### Identify a K-loci group and take it as a subset\n",
    "\n",
    "  print(serot)\n",
    "\n",
    "  # Change the label check from 0 to 1 and rename the output to switch from positive to negative interactions\n",
    "  df_onehost = sero_phage[(sero_phage[\"Best match type\"] == serot) & (sero_phage[\"label\"] == 1)]\n",
    "\n",
    "  # Only keep the first occurrence of each protein per phage\n",
    "  df_onehost = df_onehost.drop_duplicates(subset=[\"phage_ID\", \"protein_sequence\"], keep=\"first\")\n",
    "\n",
    "  # Group by phage_ID and concatenate protein sequences\n",
    "  df_grouped = df_onehost.groupby(\"phage_ID\")[\"protein_sequence\"].apply(lambda x: \"\".join(x)).reset_index()\n",
    "\n",
    "  # Rename column for clarity\n",
    "  df_grouped.rename(columns={\"protein_sequence\": \"concatenated_sequence\"}, inplace=True)\n",
    "\n",
    "\n",
    "  ### Load the file where the motifs found are identified\n",
    "\n",
    "  path = \"/motifs_KO/\" +  serot\n",
    "\n",
    "  extension = \".txt\"\n",
    "\n",
    "  files = [f for f in os.listdir(path) if f.endswith(extension)]\n",
    "\n",
    "  # If no motifs found -> no .txt file -> next iteration\n",
    "  if len(files) < 1:\n",
    "    continue\n",
    "\n",
    "  # Path to your file\n",
    "  file_path = os.path.join(path, files[0])\n",
    "\n",
    "  # Initialize a list to store extracted data\n",
    "  extracted_data = []\n",
    "\n",
    "  # Open and read the file\n",
    "  with open(file_path, \"r\") as file:\n",
    "      lines = file.readlines()\n",
    "\n",
    "  # Process the file line by line\n",
    "  for i in range(len(lines)):\n",
    "      line = lines[i].strip()\n",
    "\n",
    "      # Check if the line starts with \">\"\n",
    "      if line.startswith(\">\"):\n",
    "          # Extract the identifier before \"_\"\n",
    "          identifier = line.split(\"_\")[0][1:]  # Remove \">\"\n",
    "\n",
    "          # Extract the offset number using regex\n",
    "          offset_match = re.search(r\"offset=\\s*(\\d+)\", line)\n",
    "          offset = offset_match.group(1) if offset_match else None\n",
    "\n",
    "          # Get the next line (sequence) and compute its length\n",
    "          if i + 1 < len(lines):\n",
    "              sequence = lines[i + 1].strip()\n",
    "              seq_length = len(sequence)\n",
    "\n",
    "          # Store extracted data\n",
    "          extracted_data.append((identifier, offset, seq_length, sequence))\n",
    "\n",
    "  ### Dataset that shows where the motif was found within the concatenated proteins\n",
    "\n",
    "  # Display results\n",
    "  df_extracted = pd.DataFrame(extracted_data, columns=[\"Identifier\", \"Offset\", \"Sequence Length\", \"motif\"])\n",
    "\n",
    "  df_extracted.Offset = df_extracted.Offset.apply(lambda x: int(x))\n",
    "\n",
    "  all_extracted = pd.concat([all_extracted, df_extracted], ignore_index=True)\n",
    "\n",
    "  ### Dataset that keeps track of the start and end of each protein before they get concatenated\n",
    "\n",
    "  df_onehost_length = compute_sequential_identifier(df_onehost)\n",
    "\n",
    "  # Initialize the new column as False\n",
    "  df_onehost_length[\"motif_found\"] = False\n",
    "  df_onehost_length[\"Offset\"] = -1\n",
    "\n",
    "  ### Add \"motif_found\" == True to the row of the protein that contains the motif\n",
    "\n",
    "  # Iterate over extracted_df and find matching rows in df\n",
    "  for _, row in df_extracted.iterrows():\n",
    "      identifier = row[\"Identifier\"]\n",
    "      offset = row[\"Offset\"]\n",
    "      seq_length = row[\"Sequence Length\"]\n",
    "      target_range = (offset, offset + seq_length)  # Range for motif\n",
    "\n",
    "      # Filter rows where phage_ID matches Identifier\n",
    "      matching_rows = df_onehost_length[df_onehost_length[\"phage_ID\"] == identifier]\n",
    "\n",
    "      # Iterate through matching rows to check range inclusion\n",
    "      for index, match_row in matching_rows.iterrows():\n",
    "          start, end = map(int, match_row[\"sequential_protein_identifier\"].split(\"-\"))\n",
    "\n",
    "          # Check if the target range is within this row's sequential identifier range\n",
    "          if start <= target_range[0] and end >= target_range[1]:\n",
    "              df_onehost_length.at[index, \"motif_found\"] = True\n",
    "              df_onehost_length.at[index, \"Offset\"] = offset\n",
    "              break  # Ensure only one row is marked True per Identifier\n",
    "\n",
    "  if first_iteration:\n",
    "    full_onehost_length = copy.deepcopy(df_onehost_length)\n",
    "    first_iteration = False\n",
    "  else:\n",
    "    full_onehost_length = pd.concat([full_onehost_length, df_onehost_length], ignore_index=True)\n",
    "\n",
    "\n",
    "full_onehost_length.drop_duplicates(inplace = True)\n",
    "\n",
    "full_onehost_length.to_csv(\"full_onehost_length.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqjLgbaJ6NoU"
   },
   "outputs": [],
   "source": [
    "all_extracted.to_csv(\"all_extracted_motifs.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "ubj2TYfX08qt",
    "outputId": "83d2b174-b98a-4901-e5e7-57dcd60f7eeb"
   },
   "outputs": [],
   "source": [
    "full_onehost_length[full_onehost_length.motif_found]\n",
    "# 143 examples of proteins with motifs found\n",
    "# 28 K-loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "id": "Akcmpt9O9gLd",
    "outputId": "dd176663-a94d-40ee-c598-4a02beb167a9"
   },
   "outputs": [],
   "source": [
    "full_onehost_found = full_onehost_length[full_onehost_length.motif_found]\n",
    "\n",
    "\n",
    "# Function to calculate percentage_offset\n",
    "def calculate_percentage_offset(row):\n",
    "    try:\n",
    "        # Extract start and end from sequential_protein_identifier\n",
    "        start, end = map(int, row[\"sequential_protein_identifier\"].split(\"-\"))\n",
    "        offset = row[\"Offset\"]\n",
    "\n",
    "        # Ensure the offset is within the range\n",
    "        if start <= offset <= end:\n",
    "            return ((offset - start) / (end - start)) * 100\n",
    "    except:\n",
    "        return None  # Handle conversion errors or missing values\n",
    "\n",
    "    return None  # Default case if not computed\n",
    "\n",
    "# Apply the function to compute percentage_offset\n",
    "full_onehost_found[\"percentage_offset\"] = full_onehost_found.apply(calculate_percentage_offset, axis=1)\n",
    "\n",
    "full_onehost_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OaeOk_6o93GL",
    "outputId": "b2deee24-0962-4674-fb9d-2f1c3722d413"
   },
   "outputs": [],
   "source": [
    "print(full_onehost_found.percentage_offset.mean())\n",
    "print(full_onehost_found.percentage_offset.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JymUwMkV_Ozf",
    "outputId": "94edfe99-ab96-4dad-e7fa-82128e62428d"
   },
   "outputs": [],
   "source": [
    "print(\"Mean motif length: \", all_extracted.motif.apply(lambda x: len(x)).mean())\n",
    "print(\"motif std: \", all_extracted.motif.apply(lambda x: len(x)).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2_rbHXS_jrt",
    "outputId": "8766df86-803c-4d83-fc7c-996bdd58db69"
   },
   "outputs": [],
   "source": [
    "len(all_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlRzAvVv_g8Q",
    "outputId": "b83b4dbe-ec67-49c7-c330-b4a53472cb3d"
   },
   "outputs": [],
   "source": [
    "min(all_extracted.motif.apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "5OVQnvxl_2aW",
    "outputId": "d8562aa6-d606-4454-908a-65a262b09530"
   },
   "outputs": [],
   "source": [
    "full_onehost_found[\"protein_length\"] = full_onehost_found.protein_sequence.apply(lambda x: len(x))\n",
    "\n",
    "full_onehost_found[\"protein_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYXJZ48JBVoa"
   },
   "outputs": [],
   "source": [
    "full_onehost_found.to_csv(\"full_onehost_found.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPt7JV3KeFrZ"
   },
   "source": [
    "# AlphaFold Motif Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGwwc56EeFS9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"full_onehost_found.csv\")\n",
    "motifs = pd.read_csv(\"all_extracted_motifs.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "df[\"Best match type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oTxPG5xePq2"
   },
   "outputs": [],
   "source": [
    "df[df[\"Best match type\"] == \"K29\"].protein_ID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByiXHrwJeTrR"
   },
   "outputs": [],
   "source": [
    "prot = \"K69PH164C2_gp22\"\n",
    "serot = \"K29\"\n",
    "\n",
    "acc_K64 = df[df[\"Best match type\"] == serot][df[\"protein_ID\"] == prot][\"phage_ID\"].iloc[0]\n",
    "offs_K64 = df[df[\"Best match type\"] == serot][df[\"protein_ID\"] == prot][\"Offset\"].iloc[0]\n",
    "mot_len = motifs[motifs['Identifier'] == acc_K64][motifs[\"Offset\"] == int(offs_K64)][\"Sequence Length\"].iloc[0]\n",
    "\n",
    "\n",
    "print(\"Protein: \", prot)\n",
    "print(\"phage_ID: \", acc_K64)\n",
    "print(\"Offset: \", offs_K64)\n",
    "print(\"Motif Length: \", mot_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ad6iyn6-eZ9l"
   },
   "outputs": [],
   "source": [
    "print(prot)\n",
    "\n",
    "protein_seq = df[df[\"Best match type\"] == serot][df[\"protein_ID\"] == prot][\"protein_sequence\"].iloc[0]\n",
    "\n",
    "print(protein_seq)\n",
    "# Upload this protein in trimeric form to AlphaFold:\n",
    "# https://alphafoldserver.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wnCU7dmeezI"
   },
   "outputs": [],
   "source": [
    "protein_start = int(df[df[\"Best match type\"] == serot][df[\"protein_ID\"] == prot][\"sequential_protein_identifier\"].iloc[0].split(\"-\")[0])\n",
    "print(\"Motif Start: \", offs_K64 - protein_start)\n",
    "print(\"Motif End: \", offs_K64 - protein_start + mot_len)\n",
    "print(protein_seq[offs_K64 - protein_start:offs_K64 - protein_start + mot_len])\n",
    "# Look for position Motif Start:Motif End in Mol Viewer\n",
    "# https://molstar.org/viewer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9CpQduQJIEU"
   },
   "source": [
    "# Validating results: protein with motif found vs protein max score via tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vNHx3Meej6U",
    "outputId": "06cdfde7-13dd-498a-9f87-ff8511aed38d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLbF3y5zJQ-F"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/PhageHostLearn_Data/full_onehost_found.csv\")\n",
    "motifs = pd.read_csv(\"/content/drive/MyDrive/PhageHostLearn_Data/all_extracted_motifs.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "april_single = pd.read_csv(\"/content/drive/MyDrive/PhageHostLearn_Data/April_validation_predictions_all_groups.csv\") # 1.00 grouping threshold\n",
    "# april_single = pd.read_csv(\"/content/drive/MyDrive/PhageHostLearn_Data/validation_predictions_all_groups_75.csv\") # 0.75 grouping threshold\n",
    "# april_single = pd.read_csv(\"/content/drive/MyDrive/PhageHostLearn_Data/validation_predictions_motif_focus_100.csv\") # trained only on one protein, 1.00 grouping threshold\n",
    "# april_single = pd.read_csv(\"/content/drive/MyDrive/PhageHostLearn_Data/validation_predictions_motif_focus_80.csv\") # trained only on one protein, .80 grouping threshold\n",
    "\n",
    "# df[\"Best match type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bbj-a3PB1kL-",
    "outputId": "94b3dac6-4066-46ec-b4cd-655dfbb61f66"
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "mlcyOqddJyVe",
    "outputId": "f2feb860-b65a-49e6-f075-76c7425e1329"
   },
   "outputs": [],
   "source": [
    "motifs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "gNuiuW6uJrnF",
    "outputId": "adfe3765-5b58-4b8e-e9dd-137853e2a5f8"
   },
   "outputs": [],
   "source": [
    "print(len(df)) # 134 phage-host pairs, with the motif found in protein_ID\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "HZ0tEZaBJuIu",
    "outputId": "4e9b1707-f725-4bb8-ff91-f1fcaf033cb1"
   },
   "outputs": [],
   "source": [
    "april_single.head(3)\n",
    "# 25'120 accession-phage-protein rows\n",
    "\n",
    "# focus on successful interactions only (label=1.0)\n",
    "# Add a score ranking grouped by accession-phage_ID, max score is #1\n",
    "# Check how many times the protein with the motif is #1, how many times top-n\n",
    "# Find way to normalize by number of phages within that phage (es: finding the top protein when only 1, not impressive.\n",
    "# out of 8, impressive. Find way to check this relative position. 1/1 = 0, 1/8 = max, perhaps just number of proteins\n",
    "# minus position over number of proteins, minus the average?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ME5So885KbBV",
    "outputId": "7388edbf-c8cb-4dcf-9101-bd3b471db177"
   },
   "outputs": [],
   "source": [
    "april_single['score_ranking'] = april_single.groupby(['accession', 'phage_ID'])['score'] \\\n",
    "                        .rank(method='dense', ascending=False) \\\n",
    "                        .astype(int)\n",
    "\n",
    "successful_april = april_single[april_single.label == 1]\n",
    "# successful_april = april_single[april_single.true_label == 1]\n",
    "\n",
    "successful_april.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbUp77UQTenl",
    "outputId": "b6c228e1-37f4-40d0-ff13-3a3aa1b37ee6"
   },
   "outputs": [],
   "source": [
    "# Step 1: Count number of unique protein_IDs per (accession, phage_ID) in the first dataframe\n",
    "protein_counts = successful_april.groupby(['accession', 'phage_ID'])['protein_ID'].nunique()\n",
    "\n",
    "# Step 2: Convert the index to a dictionary\n",
    "protein_count_dict = protein_counts.to_dict()\n",
    "\n",
    "# Step 3: Map the dictionary to a new column in the second dataframe\n",
    "successful_april['protein_count'] = successful_april.apply(\n",
    "    lambda row: protein_count_dict.get((row['accession'], row['phage_ID']), 0),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "for i in range(2,9):\n",
    "  # Group by accession and phage_ID, compute standard deviation of scores per group\n",
    "  std_per_group = successful_april[successful_april[\"protein_count\"] == i].groupby(['accession', 'phage_ID'])['score'].std()\n",
    "\n",
    "  # Then take the average of all those standard deviations\n",
    "  average_std = std_per_group.mean()\n",
    "\n",
    "  print(i)\n",
    "  print(\"Average standard deviation of scores per (accession, phage_ID):\", average_std)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXRDnWefTZIV",
    "outputId": "c133d68c-e691-4a2e-f7d2-8c62ad941142"
   },
   "outputs": [],
   "source": [
    "successful_april.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "dBqO55WQNxQ3",
    "outputId": "d2a65633-77c1-43e8-e760-b19c80e13164"
   },
   "outputs": [],
   "source": [
    "successful_april[successful_april[\"accession\"] == \"KP_HGUA02_071\"][successful_april[\"phage_ID\"] == \"A3d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "d5942CNGNyUA",
    "outputId": "fec03b47-dd5c-4841-a2c7-5eca0be1f8c8"
   },
   "outputs": [],
   "source": [
    "top_successful_april = successful_april[successful_april[\"score_ranking\"] == 1]\n",
    "# 333 phage-host couples\n",
    "top_successful_april.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "PIwA_OBJOara",
    "outputId": "c0ad7a01-0a9b-42aa-f94c-08bf8c3dce8f"
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "kGisclAP1POi",
    "outputId": "35162ab6-f52a-4e4b-913f-29ecad9fa7e6"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbufEJe5QCen"
   },
   "outputs": [],
   "source": [
    "# Merge the score_ranking column from successful_april into df2\n",
    "df_scores = df.merge(\n",
    "    successful_april[['accession', 'phage_ID', 'protein_ID', 'score_ranking']],\n",
    "    on=['accession', 'phage_ID', 'protein_ID'],\n",
    "    how='left'  # Keeps all rows in df, adds score_ranking where there's a match\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJasZmofRyP8"
   },
   "outputs": [],
   "source": [
    "# Step 1: Count number of unique protein_IDs per (accession, phage_ID) in the first dataframe\n",
    "protein_counts = successful_april.groupby(['accession', 'phage_ID'])['protein_ID'].nunique()\n",
    "\n",
    "# Step 2: Convert the index to a dictionary\n",
    "protein_count_dict = protein_counts.to_dict()\n",
    "\n",
    "# Step 3: Map the dictionary to a new column in the second dataframe\n",
    "df_scores['protein_count'] = df_scores.apply(\n",
    "    lambda row: protein_count_dict.get((row['accession'], row['phage_ID']), 0),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "lb8MZ8v6R6T1",
    "outputId": "b2b324b9-ba39-4c86-8c7b-796b114c6453"
   },
   "outputs": [],
   "source": [
    "df_scores.protein_count.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "X66iziJKQuLs",
    "outputId": "5b34bb84-fdec-4a3c-9d63-5c8924f1cab6"
   },
   "outputs": [],
   "source": [
    "df_scores.score_ranking.value_counts()\n",
    "# 18 cases with protein_count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "tqnBv9v4Q0Qh",
    "outputId": "d0bd47b9-2026-4aa9-b46d-37f69200addb"
   },
   "outputs": [],
   "source": [
    "df_scores[df_scores[\"protein_count\"] == 5].score_ranking.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j6mbuCH7nZ4b",
    "outputId": "ec38a369-e244-4fbd-b806-2ae8c7f87713"
   },
   "outputs": [],
   "source": [
    "df_scores[df_scores[\"protein_count\"] == 2].score_ranking.value_counts().loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Ec8yZ2OeJOY",
    "outputId": "234c2f11-0475-403e-9c88-1831413bb61b"
   },
   "outputs": [],
   "source": [
    "print(\"***** RESULTS AT 1.00 SIMILARITY GROUPING *****\")\n",
    "print()\n",
    "\n",
    "group_sizes = []\n",
    "num_phages = []\n",
    "num_correct = []\n",
    "\n",
    "for i in range(2, 9):\n",
    "  if sum(df_scores[df_scores[\"protein_count\"] == i].score_ranking.value_counts()) != 0:\n",
    "    print(\"Number of proteins per phage: \", i)\n",
    "    print(\"Total count: \", sum(df_scores[df_scores[\"protein_count\"] == i].score_ranking.value_counts()))\n",
    "\n",
    "    first_count = 0\n",
    "    try:\n",
    "      first_count = df_scores[df_scores[\"protein_count\"] == i].score_ranking.value_counts().loc[1]\n",
    "    except:\n",
    "      pass\n",
    "    group_sizes.append(i)\n",
    "    num_phages.append(sum(df_scores[df_scores[\"protein_count\"] == i].score_ranking.value_counts()))\n",
    "    num_correct.append(first_count)\n",
    "    print(num_correct)\n",
    "    try:\n",
    "      print(\"First position: \", round( first_count / (sum(df_scores[df_scores[\"protein_count\"] == i].score_ranking.value_counts())), 3))\n",
    "      print(first_count)\n",
    "      print(\"Expected if Random: \", (1 / i))\n",
    "    except ZeroDivisionError:\n",
    "      print(\"DivisionByZero\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJwnCjYBeYEQ",
    "outputId": "b556dbb5-78d5-45c0-a877-f7f142025875"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "\n",
    "# Example data (replace with your actual values)\n",
    "# group_sizes = [2, 3, 4, 5]           # number of proteins per phage\n",
    "# num_phages = [37, 28, 25, 30]        # number of phages in each group\n",
    "# num_correct = [15, 10, 8, 7]         # correct identifications in each group\n",
    "\n",
    "expected_raw = [k / n for k, n in zip(num_phages, group_sizes)]\n",
    "\n",
    "# Normalize expected to match the total of observed\n",
    "total_observed = sum(num_correct)\n",
    "total_expected = sum(expected_raw)\n",
    "scaling_factor = total_observed / total_expected\n",
    "expected_scaled = [e * scaling_factor for e in expected_raw]\n",
    "\n",
    "# Chi-squared test\n",
    "chi2_stat, p_value = chisquare(f_obs=num_correct, f_exp=expected_scaled)\n",
    "\n",
    "print(f\"Chi-squared statistic: {chi2_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CcNr1sXeIzZ",
    "outputId": "9205162e-cad6-49d2-f783-314be910aa43"
   },
   "outputs": [],
   "source": [
    "group_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbZCc8qAS2P4",
    "outputId": "35f4330f-d2bc-4f8e-c647-0c559569698e"
   },
   "outputs": [],
   "source": [
    "num_phages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gd5r_YHpkhlj",
    "outputId": "2eab5f8f-196d-400b-aeb0-0b6f5fa5d8c9"
   },
   "outputs": [],
   "source": [
    "num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7w3FOXEl7RH",
    "outputId": "916e648b-241e-4a5f-96ba-3284ecb2670b"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binomtest\n",
    "\n",
    "# Your data\n",
    "group_sizes = [2, 3, 4, 5, 6, 8]\n",
    "num_phages = [37, 28, 3, 42, 5, 1]\n",
    "# num_correct = [32, 16, 1, 24, 4, 0]\n",
    "num_correct = [15, 10, 3, 9, 1, 0]\n",
    "\n",
    "# Run binomial tests\n",
    "for n, k, x in zip(group_sizes, num_phages, num_correct):\n",
    "    p_null = 1 / n  # random guessing probability\n",
    "    result = binomtest(k=x, n=k, p=p_null, alternative='greater')  # testing if better than random\n",
    "    print(f\"{n} proteins: {x}/{k} correct (random = {p_null:.2f}) -> p = {result.pvalue:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Cm-lZFUkVEi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhNzc49U_aes"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhXY8eg6q7s_",
    "outputId": "970ff35e-1492-4509-c17d-ffc8ba2b438b"
   },
   "outputs": [],
   "source": [
    "num_phages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x9mTYXv6UZvP",
    "outputId": "3a37e717-b8b1-415f-e486-b4eba6b29175"
   },
   "outputs": [],
   "source": [
    "[int(x) for x in num_correct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycoX8l_gUalL",
    "outputId": "15a83595-fd6f-4f29-c8d9-f025aa669bf6"
   },
   "outputs": [],
   "source": [
    "group_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QM4wAZ5yxC0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbe7dMg6swl5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
